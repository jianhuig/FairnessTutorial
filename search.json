[{"path":"https://jianhuig.github.io/FairnessTutorial/articles/MultiGroup.html","id":"model-building","dir":"Articles","previous_headings":"","what":"Model Building","title":"MultiGroup","text":"","code":"data <- PheCAP::PhecapData(ehr_data, \"healthcare_utilization\", \"label\", 0.4)$frame # Binarize outcome (main_ICD) data <- data %>% mutate(outcome = ifelse(main_ICD > 0, 1, 0)) # Split data into groups (1, 2, 3, 4) based on healthcare_utilization quantiles data <- data %>% mutate(HU_group = ifelse(healthcare_utilization <= quantile(healthcare_utilization, 0.25), 1,                                           ifelse(healthcare_utilization <= quantile(healthcare_utilization, 0.5), 2,                                                  ifelse(healthcare_utilization <= quantile(healthcare_utilization, 0.75), 3, 4)))) # Keep only interested target_data <- data %>% select(outcome, main_NLP, HU_group, healthcare_utilization, starts_with(\"NLP\")) # Check if any missing data sum(is.na(target_data)) #> [1] 0  # 75% of the sample size train_size <- floor(0.75 * nrow(target_data)) # Split data set.seed(321) train_ind <- sample(seq_len(nrow(target_data)), size=train_size) train <- target_data[train_ind, ] x_train <- as.matrix(train %>% select(-outcome, -HU_group)) y_train <- as.matrix(train %>% select(outcome))  test <- target_data[-train_ind, ] x_test <- as.matrix(test %>% select(-outcome, -HU_group)) y_test <- test %>% select(outcome)  # Manually create 10-folds to make sure the results are reproducible set.seed(30) folds <- createFolds(y_train, k = 10, list = TRUE)  # Convert the list to a vector where each element indicates the fold number fold_ids <- rep(NA, length(y_train)) for(i in 1:10) {   fold_ids[folds[[i]]] <- i }  # To choose lambda (regularization parameter), cross-validation is typically used # Pass the fold_ids to cv.glmnet through the foldid argument cv_fit <- cv.glmnet(x_train, y_train, family = \"binomial\", alpha = 1, foldid = fold_ids)  # Best lambda value best_lambda <- cv_fit$lambda.min  # Fit logistic Regression fit <- glmnet::glmnet(x_train, y_train, family = \"binomial\", alpha=1, lambda = best_lambda)  # Predictions test$pred <- predict(fit, newx = x_test, type = \"response\", s = best_lambda)[,1] mean(ifelse(test$pred > 0.4, 1, 0) == y_test) #> [1] 0.8176  # The dataframe would need predictions, outcome, HU_group test %>%   mutate(pred = ifelse(pred > 0.4, 1, 0)) %>%   filter(outcome == 0) %>%   summarise(fpr = mean(pred)) #>         fpr #> 1 0.1363636  # Calculate the fairness metrics test <- test %>%   mutate(hu_group = ifelse(HU_group == 1, paste0(\"0 to \", quantile(data$healthcare_utilization, 0.25)),                            ifelse(HU_group ==2, paste0(quantile(data$healthcare_utilization, 0.25), \" to \", quantile(data$healthcare_utilization, 0.5)),                                   ifelse(HU_group == 3, paste0(quantile(data$healthcare_utilization, 0.5), \" to \", quantile(data$healthcare_utilization, 0.75)), paste0(quantile(data$healthcare_utilization, 0.75), \" to \", quantile(data$healthcare_utilization, 1)))))) cut_off <- 0.4 test <- test %>% select(outcome, pred, HU_group)"},{"path":[]},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/MultiGroup.html","id":"max-min-difference","dir":"Articles","previous_headings":"Multi-Group Fairness Evaluation","what":"Max-Min Difference","title":"MultiGroup","text":"","code":"eval_max_min_diff(data = test,                    outcome = \"outcome\",                    probs = \"pred\",                   group = \"HU_group\",                   bootstraps = 1000) #>          Metric Group 1 Group 2 Group 3 Group 4 Max-Min        95% CI #> 1           TPR    0.37    0.65    0.73    0.73    0.36    [0.3,0.44] #> 2           FPR    0.02    0.13    0.09    0.20    0.18   [0.13,0.24] #> 3           PPR    0.34    0.16    0.49    0.27    0.33   [0.28,0.38] #> 4           PPV    0.89    0.94    0.79    0.83    0.15    [0.1,0.22] #> 5           NPV    0.83    0.69    0.71    0.83    0.14    [0.11,0.2] #> 6           ACC    0.85    0.73    0.75    0.83    0.12   [0.09,0.17] #> 7   Brier Score    0.11    0.17    0.17    0.12    0.06   [0.05,0.09] #> 8   FN/FP Ratio    3.04   27.67    1.37    2.69   26.30 [13.31,85.39] #> 9 Avg Pred Prob    0.41    0.37    0.52    0.37    0.15   [0.12,0.18]"},{"path":[]},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"missing-data","dir":"Articles","previous_headings":"Data Preprocessing","what":"Missing Data","title":"Two Group Fairness Tutorial","text":"","code":"# Calculate the number of missing values per column missing_values <- sapply(mimic, function(x) sum(is.na(x)))  # Calculate the percentage of missing values per column missing_values_percentage <- sapply(mimic, function(x) sum(is.na(x)) / length(x) * 100)  # Combine the results into a data frame for easy viewing missing_data_summary <- data.frame(Number_of_Missing_Values = missing_values,                                     Percentage = missing_values_percentage)  # Print the summary print(missing_data_summary) #>                    Number_of_Missing_Values  Percentage #> aline_flg                                 0  0.00000000 #> icu_los_day                               0  0.00000000 #> hospital_los_day                          0  0.00000000 #> age                                       0  0.00000000 #> gender_num                                1  0.05630631 #> weight_first                            110  6.19369369 #> bmi                                     466 26.23873874 #> sapsi_first                              85  4.78603604 #> sofa_first                                6  0.33783784 #> service_unit                              0  0.00000000 #> service_num                               0  0.00000000 #> day_icu_intime                            0  0.00000000 #> day_icu_intime_num                        0  0.00000000 #> hour_icu_intime                           0  0.00000000 #> hosp_exp_flg                              0  0.00000000 #> icu_exp_flg                               0  0.00000000 #> day_28_flg                                0  0.00000000 #> mort_day_censored                         0  0.00000000 #> censor_flg                                0  0.00000000 #> sepsis_flg                                0  0.00000000 #> chf_flg                                   0  0.00000000 #> afib_flg                                  0  0.00000000 #> renal_flg                                 0  0.00000000 #> liver_flg                                 0  0.00000000 #> copd_flg                                  0  0.00000000 #> cad_flg                                   0  0.00000000 #> stroke_flg                                0  0.00000000 #> mal_flg                                   0  0.00000000 #> resp_flg                                  0  0.00000000 #> map_1st                                   0  0.00000000 #> hr_1st                                    0  0.00000000 #> temp_1st                                  3  0.16891892 #> spo2_1st                                  0  0.00000000 #> abg_count                                 0  0.00000000 #> wbc_first                                 8  0.45045045 #> hgb_first                                 8  0.45045045 #> platelet_first                            8  0.45045045 #> sodium_first                              5  0.28153153 #> potassium_first                           5  0.28153153 #> tco2_first                                5  0.28153153 #> chloride_first                            5  0.28153153 #> bun_first                                 5  0.28153153 #> creatinine_first                          6  0.33783784 #> po2_first                               186 10.47297297 #> pco2_first                              186 10.47297297 #> iv_day_1                                143  8.05180180  # remove columns with more than 10% missing data and impute the rest with median # Identify columns with more than 10% missing values columns_to_remove <- names(missing_values_percentage[missing_values_percentage > 10])  # Remove these columns mimic <- select(mimic, -one_of(columns_to_remove))  # Impute remaining missing values with median mimic<- mimic %>% mutate(across(where(~any(is.na(.))), ~ifelse(is.na(.), median(., na.rm = TRUE), .)))  # Check if there are any missing values left remaining_missing_values <- sum(sapply(mimic, function(x) sum(is.na(x)))) remaining_missing_values #> [1] 0  # Identify columns that have only one unique value cols_with_one_value <- sapply(mimic, function(x) length(unique(x)) == 1)  # Subset the dataframe to remove these columns mimic <- mimic[, !cols_with_one_value]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"model-building","dir":"Articles","previous_headings":"Data Preprocessing","what":"Model Building","title":"Two Group Fairness Tutorial","text":"","code":"# Remove columns that are highly correlated with the outcome variable corrplot(cor(select_if(mimic, is.numeric)), method = \"color\", tl.cex = 0.5) mimic <- mimic %>%    select(-c(\"hosp_exp_flg\", \"icu_exp_flg\", \"mort_day_censored\", \"censor_flg\"))  # Use 700 labels to train the mimic train_data <- mimic %>% filter(row_number() <= 700) # Fit a random forest model rf_model <- randomForest(factor(day_28_flg) ~ ., data = train_data, seed = 123)  # Test the model on the remaining data test_data <- mimic %>% filter(row_number() > 700) test_data$pred <- predict(rf_model, newdata = test_data, type = \"prob\")[,2]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"fairness-evaluation","dir":"Articles","previous_headings":"","what":"Fairness Evaluation","title":"Two Group Fairness Tutorial","text":"use sex sensitive attribute day_28_flg outcome. choose threshold = 0.41 overall FPR around 5%.","code":"test_data <- test_data %>%   mutate(gender = ifelse(gender_num == 1, \"Male\", \"Female\")) cut_off <- 0.41  test_data %>%   mutate(pred = ifelse(pred > 0.41, 1, 0)) %>%   filter(day_28_flg == 0) %>%   summarise(fpr = mean(pred)) #>          fpr #> 1 0.05604396"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"equal-opportunity","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Equal Opportunity","title":"Two Group Fairness Tutorial","text":"","code":"eval_eq_opp(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is evidence that model does not satisfy equal opportunity. #>   Metric GroupMale GroupFemale Difference       95% CI #> 1    TPR      0.64        0.48       0.16 [0.01, 0.31]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"equalized-odds","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Equalized Odds","title":"Two Group Fairness Tutorial","text":"","code":"eval_eq_odds(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is evidence that model does not satisfy equalized odds. #>   Metric Group Male Group Female Difference        95% CI #> 1    TPR       0.64         0.48       0.16  [0.01, 0.31] #> 2    FPR       0.41         0.23       0.18 [-0.02, 0.38]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"statistical-parity","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Statistical Parity","title":"Two Group Fairness Tutorial","text":"","code":"eval_stats_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is evidence that model does not satisfy statistical parity. #>   Metric Group Male Group Female Difference         95% CI #> 1    PPR        0.1         0.18      -0.08 [-0.12, -0.04]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"conditional-statistical-parity","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Conditional Statistical Parity","title":"Two Group Fairness Tutorial","text":"conditional age >= 60. can also condition categorical variable. example, can condition service unit = MICU.","code":"eval_cond_stats_parity(dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off,   group2 = \"age\",   condition = \">= 60\") #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>   Metric Group Female Group Male Difference        95% CI #> 1    PPR         0.35       0.27       0.08 [-0.01, 0.17] eval_cond_stats_parity(dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off,   group2 = \"service_unit\",   condition = \"MICU\") #> There is not enough evidence that the model does not satisfy #>             statistical parity. #>   Metric Group Female Group Male Difference        95% CI #> 1    PPR         0.16       0.12       0.04 [-0.03, 0.11]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"predictive-parity","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Predictive Parity","title":"Two Group Fairness Tutorial","text":"","code":"eval_pred_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is not enough evidence that the model does not satisfy #>             predictive parity. #>   Metric GroupMale GroupFemale Difference        95% CI #> 1    PPV      0.68        0.61       0.07 [-0.08, 0.22]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"predictive-equality","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Predictive Equality","title":"Two Group Fairness Tutorial","text":"","code":"eval_pred_equality(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is not enough evidence that the model does not satisfy #>             predictive equality. #>   Metric GroupMale GroupFemale Difference        95% CI #> 1    FPR      0.41        0.23       0.18 [-0.02, 0.38]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"conditional-accuracy-equality","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Conditional Accuracy Equality","title":"Two Group Fairness Tutorial","text":"","code":"eval_cond_acc_equality(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is not enough evidence that the model does not satisfy #>             conditional use accuracy equality. #>   Metric GroupMale GroupFemale Difference        95% CI #> 1    PPV      0.68        0.61       0.07 [-0.09, 0.23] #> 2    NPV      0.92        0.92       0.00 [-0.04, 0.04]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"accuracy-parity","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Accuracy Parity","title":"Two Group Fairness Tutorial","text":"","code":"eval_acc_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\",   cutoff = cut_off ) #> There is not enough evidence that the model does not satisfy #>             accuracy parity. #>     Metric GroupMale GroupFemale Difference        95% CI #> 1 Accuracy      0.89        0.87       0.02 [-0.02, 0.06]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"brier-score-parity","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Brier Score Parity","title":"Two Group Fairness Tutorial","text":"","code":"eval_bs_parity(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is not enough evidence that the model does not satisfy #>             Brier Score parity. #>        Metric GroupMale GroupFemale Difference        95% CI #> 1 Brier Score      0.08        0.09      -0.01 [-0.03, 0.01]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"treatment-equality","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Treatment Equality","title":"Two Group Fairness Tutorial","text":"","code":"eval_treatment_equality(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> [1] 11.17  4.55 #> There is not enough evidence that the model does not satisfy #>             treatment equality. #>        Metric GroupMale GroupFemale Difference          95% CI #> 1 FN/FP Ratio     11.17        4.55       6.62 [-13.21, 26.45]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"balance-for-positive-class","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Balance for Positive Class","title":"Two Group Fairness Tutorial","text":"","code":"eval_pos_class_bal(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is evidence that the model does not satisfy #>             balance for positive class. #>                 Metric GroupMale GroupFemale Difference       95% CI #> 1 Avg. Predicted Prob.      0.46        0.37       0.09 [0.04, 0.14]"},{"path":"https://jianhuig.github.io/FairnessTutorial/articles/TwoGroup.html","id":"balance-for-negative-class","dir":"Articles","previous_headings":"Fairness Evaluation","what":"Balance for Negative Class","title":"Two Group Fairness Tutorial","text":"","code":"eval_neg_class_bal(   dat = test_data,   outcome = \"day_28_flg\",   group = \"gender\",   probs = \"pred\" ) #> There is enough evidence that the model does not satisfy #>             balance for negative class. #>                 Metric GroupMale GroupFemale Difference         95% CI #> 1 Avg. Predicted Prob.      0.11        0.15      -0.04 [-0.06, -0.02]"},{"path":"https://jianhuig.github.io/FairnessTutorial/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jianhui Gao. Author, maintainer.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gao J (2024). FairnessTutorial: Package (One Line, Title Case). R package version 0.0.0.9000, https://jianhuig.github.io/FairnessTutorial/.","code":"@Manual{,   title = {FairnessTutorial: What the Package Does (One Line, Title Case)},   author = {Jianhui Gao},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://jianhuig.github.io/FairnessTutorial/}, }"},{"path":"https://jianhuig.github.io/FairnessTutorial/index.html","id":"fairnesstutorial","dir":"","previous_headings":"","what":"What the Package Does (One Line, Title Case)","title":"What the Package Does (One Line, Title Case)","text":"R package Fairness Tutorial","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"What the Package Does (One Line, Title Case)","text":"{R, eval = FALSE} devtools::install_github(repo = \"https://github.com/jianhuig/FairnessTutorial\")","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_acc_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine accuracy parity of a model — eval_acc_parity","title":"Examine accuracy parity of a model — eval_acc_parity","text":"Examine accuracy parity model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_acc_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine accuracy parity of a model — eval_acc_parity","text":"","code":"eval_acc_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_acc_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine accuracy parity of a model — eval_acc_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_acc_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine accuracy parity of a model — eval_acc_parity","text":"list containing following elements: Accuracy Group 1 Accuracy Group 2 Difference accuracy confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference accuracy","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_bs_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Brier Score parity of a model — eval_bs_parity","title":"Examine Brier Score parity of a model — eval_bs_parity","text":"Examine Brier Score parity model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_bs_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Brier Score parity of a model — eval_bs_parity","text":"","code":"eval_bs_parity(   data,   outcome,   group,   probs,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_bs_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Brier Score parity of a model — eval_bs_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_bs_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Brier Score parity of a model — eval_bs_parity","text":"list containing following elements: Brier Score Group 1 Brier Score Group 2 Difference Brier Score confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference Brier Score","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_acc_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine conditional use accuracy equality of a model — eval_cond_acc_equality","title":"Examine conditional use accuracy equality of a model — eval_cond_acc_equality","text":"Examine conditional use accuracy equality model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_acc_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine conditional use accuracy equality of a model — eval_cond_acc_equality","text":"","code":"eval_cond_acc_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_acc_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine conditional use accuracy equality of a model — eval_cond_acc_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_acc_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine conditional use accuracy equality of a model — eval_cond_acc_equality","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value NPV_Group1: Negative Predictive Value first group NPV_Group2: Negative Predictive Value second group NPV_Diff: Difference Negative Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value NPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Negative Predictive Value","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine conditional statistical parity of a model — eval_cond_stats_parity","title":"Examine conditional statistical parity of a model — eval_cond_stats_parity","text":"Examine conditional statistical parity model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine conditional statistical parity of a model — eval_cond_stats_parity","text":"","code":"eval_cond_stats_parity(   data,   outcome,   group,   group2,   condition,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   message = TRUE,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine conditional statistical parity of a model — eval_cond_stats_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute group2 Name group condition condition conditional group categorical, condition supplied must character levels condition . conditional group continuous, conditions supplied must character containing sign condition value threshold continuous variable (e.g. \"<50\", \">50\", \"<=50\", \">=50\"). probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 message Whether print results, default TRUE digits Number digits round results , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_cond_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine conditional statistical parity of a model — eval_cond_stats_parity","text":"list containing following elements: Conditions: conditions used calculate conditional PPR PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_odds.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"function examines equalized odds predictive model comparing True Positive Rates (TPR) False Positive Rates (FPR) across different groups defined sensitive attribute. assesses model performs unbiasedly binary outcomes across groups, adhering equalized odds fairness criterion.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_odds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"eval_eq_odds(   data,   outcome,   group,   probs,   cutoff = 0.5,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_odds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"data dataframe containing actual outcomes, predicted outcomes, sensitive attributes necessary evaluating model fairness. outcome name outcome variable data; must binary. group name sensitive attribute variable used define groups comparison fairness evaluation. probs name variable containing predicted probabilities scores. cutoff threshold converting predicted probabilities binary predictions; defaults 0.5. bootstraps number bootstrap samples used estimating uncertainty fairness metrics; defaults 1000. digits number decimal places numerical results rounded; defaults 2. message Logical; whether print summary results console; defaults TRUE.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_odds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"Returns dataframe following columns: Metric: Describes metric reported (TPR FPR group, difference). Group1: Rate first group. Group2: Rate second group. Difference: difference rates two groups. 95% CI: 95% confidence interval rate difference。","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_odds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Examine Equalized Odds of a Predictive Model — eval_eq_odds","text":"","code":"# Example usage: eval_eq_odds(   data = your_data, outcome = \"actual_outcome\",   group = \"sensitive_attribute\", probs = \"predicted_probs\" ) #> Error in eval(expr, envir, enclos): object 'your_data' not found"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_opp.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"function evaluates equal opportunity compliance predictive model comparing True Positive Rates (TPR) across different groups defined sensitive attribute. used determine model exhibits bias towards group binary outcomes.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_opp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"eval_eq_opp(   data,   outcome,   group,   probs,   cutoff = 0.5,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_opp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"data dataframe containing actual outcomes, predicted probabilities, sensitive attributes necessary evaluating model fairness. outcome name outcome variable data; must binary. group name sensitive attribute variable used define groups comparison fairness evaluation. probs name variable containing predicted probabilities scores. cutoff threshold converting predicted probabilities binary predictions; defaults 0.5. bootstraps number bootstrap samples used estimating confidence interval; defaults 1000. digits number decimal places numerical results rounded; defaults 2. message Logical; whether print summary results console; defaults TRUE.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_opp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"Returns dataframe following columns: Metric: Describes metric reported (TPR group, difference). Group1: True Positive Rate first group. Group2: True Positive Rate second group. Difference: difference True Positive Rates two groups. CI: 95% confidence interval TPR difference.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_eq_opp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluate Equal Opportunity Compliance of a Predictive Model — eval_eq_opp","text":"","code":"# Example usage: eval_eq_opp(   data = your_data, outcome = \"actual_outcome\",   group = \"sensitive_attribute\", probs = \"predicted_probs\" ) #> Error in eval(expr, envir, enclos): object 'your_data' not found"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_generalized_entropy_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Generalized Entropy Index of a model\n#' This function evaluates the generalized entropy index in model\nperformance metrics across different groups. — eval_generalized_entropy_index","title":"Examine Generalized Entropy Index of a model\n#' This function evaluates the generalized entropy index in model\nperformance metrics across different groups. — eval_generalized_entropy_index","text":"Examine Generalized Entropy Index model #' function evaluates generalized entropy index model performance metrics across different groups.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_generalized_entropy_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Generalized Entropy Index of a model\n#' This function evaluates the generalized entropy index in model\nperformance metrics across different groups. — eval_generalized_entropy_index","text":"","code":"eval_generalized_entropy_index(   data,   outcome,   group,   probs,   alpha = 2,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_generalized_entropy_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Generalized Entropy Index of a model\n#' This function evaluates the generalized entropy index in model\nperformance metrics across different groups. — eval_generalized_entropy_index","text":"data Data frame containing outcome, predicted outcome, group outcome Name outcome variable, must binary group Name group probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_generalized_entropy_index.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Generalized Entropy Index of a model\n#' This function evaluates the generalized entropy index in model\nperformance metrics across different groups. — eval_generalized_entropy_index","text":"data frame containing following elements: Metric: names metrics calculated. Values group. Generalized_entropy_index: generalized entropy index model confidence intervals computed (confint = TRUE): 95% CI: string providing lower upper bounds 95% confidence interval generalized entropy index.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_abs_diff.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine max absolute difference of a model — eval_max_abs_diff","title":"Examine max absolute difference of a model — eval_max_abs_diff","text":"function evaluates maximum absolute difference model performance metrics across different groups.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_abs_diff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine max absolute difference of a model — eval_max_abs_diff","text":"","code":"eval_max_abs_diff(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_abs_diff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine max absolute difference of a model — eval_max_abs_diff","text":"data Data frame containing outcome, predicted outcome, group outcome Name outcome variable, must binary group Name group probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_abs_diff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine max absolute difference of a model — eval_max_abs_diff","text":"data frame containing following elements: Metric: names metrics calculated. Values group. Max_abs_diff: maximum absolute difference model confidence intervals computed (confint = TRUE): 95% CI: string providing lower upper bounds 95% confidence interval maximum absolute difference.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_diff.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Maximum-Minimum Difference of a Model — eval_max_min_diff","title":"Examine Maximum-Minimum Difference of a Model — eval_max_min_diff","text":"function evaluates maximum minimum differences model performance metrics across different groups.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_diff.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Maximum-Minimum Difference of a Model — eval_max_min_diff","text":"","code":"eval_max_min_diff(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_diff.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Maximum-Minimum Difference of a Model — eval_max_min_diff","text":"data Data frame containing outcome, predicted outcome, group. outcome Name outcome variable, must binary. group Name group variable. probs Name predicted outcome variable. cutoff Threshold predicted outcome, default 0.5. confint Whether compute 95% confidence interval, default TRUE. bootstraps Number bootstrap samples, default 1000. digits Number digits round results , default 2.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_diff.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Maximum-Minimum Difference of a Model — eval_max_min_diff","text":"data frame containing following elements: Metric: names metrics calculated. Values group. Max-Min: maximum minus minimum metric difference model. confidence intervals computed (confint = TRUE): 95% CI: string providing lower upper bounds 95% confidence interval max-min metric difference.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_ratio.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine Maximum Minimum Ratio of a model — eval_max_min_ratio","title":"Examine Maximum Minimum Ratio of a model — eval_max_min_ratio","text":"function evaluates maximum minimum ratio model performance metrics across different groups.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_ratio.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine Maximum Minimum Ratio of a model — eval_max_min_ratio","text":"","code":"eval_max_min_ratio(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_ratio.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine Maximum Minimum Ratio of a model — eval_max_min_ratio","text":"data Data frame containing outcome, predicted outcome, group outcome Name outcome variable, must binary group Name group probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_max_min_ratio.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine Maximum Minimum Ratio of a model — eval_max_min_ratio","text":"data frame containing following elements: Metric: names metrics calculated. Values group. Max/Min: maximum minimum metric difference model confidence intervals computed (confint = TRUE): 95% CI: string providing lower upper bounds 95% confidence interval max/min metric difference.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_mean_abs_dev.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine mean absolute deviation of a model\n#' This function evaluates the mean absolute deviation in model\nperformance metrics across different groups. — eval_mean_abs_dev","title":"Examine mean absolute deviation of a model\n#' This function evaluates the mean absolute deviation in model\nperformance metrics across different groups. — eval_mean_abs_dev","text":"Examine mean absolute deviation model #' function evaluates mean absolute deviation model performance metrics across different groups.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_mean_abs_dev.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine mean absolute deviation of a model\n#' This function evaluates the mean absolute deviation in model\nperformance metrics across different groups. — eval_mean_abs_dev","text":"","code":"eval_mean_abs_dev(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_mean_abs_dev.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine mean absolute deviation of a model\n#' This function evaluates the mean absolute deviation in model\nperformance metrics across different groups. — eval_mean_abs_dev","text":"data Data frame containing outcome, predicted outcome, group outcome Name outcome variable, must binary group Name group probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_mean_abs_dev.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine mean absolute deviation of a model\n#' This function evaluates the mean absolute deviation in model\nperformance metrics across different groups. — eval_mean_abs_dev","text":"data frame containing following elements: Metric: names metrics calculated. Values group. Mean_abs_dev: mean absolute deviation model confidence intervals computed (confint = TRUE): 95% CI: string providing lower upper bounds 95% confidence interval mean absolute deviation.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_neg_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine balance for negative class of a model — eval_neg_class_bal","title":"Examine balance for negative class of a model — eval_neg_class_bal","text":"Examine balance negative class model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_neg_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine balance for negative class of a model — eval_neg_class_bal","text":"","code":"eval_neg_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_neg_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine balance for negative class of a model — eval_neg_class_bal","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_neg_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine balance for negative class of a model — eval_neg_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pos_class_bal.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine balance for positive class of a model — eval_pos_class_bal","title":"Examine balance for positive class of a model — eval_pos_class_bal","text":"Examine balance positive class model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pos_class_bal.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine balance for positive class of a model — eval_pos_class_bal","text":"","code":"eval_pos_class_bal(   data,   outcome,   group,   probs,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pos_class_bal.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine balance for positive class of a model — eval_pos_class_bal","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities confint Logical indicating whether calculate confidence intervals bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pos_class_bal.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine balance for positive class of a model — eval_pos_class_bal","text":"list containing following elements: Average predicted probability Group 1 Average predicted probability Group 2 Difference average predicted probability confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference average predicted probability","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine predictive equality of a model — eval_pred_equality","title":"Examine predictive equality of a model — eval_pred_equality","text":"Examine predictive equality model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine predictive equality of a model — eval_pred_equality","text":"","code":"eval_pred_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine predictive equality of a model — eval_pred_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine predictive equality of a model — eval_pred_equality","text":"list containing following elements: FPR_Group1: False Positive Rate first group FPR_Group2: False Positive Rate second group FPR_Diff: Difference False Positive Rate confidence intervals computed (confint = TRUE): FPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference False Positive Rate","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine predictive parity of a model — eval_pred_parity","title":"Examine predictive parity of a model — eval_pred_parity","text":"Examine predictive parity model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine predictive parity of a model — eval_pred_parity","text":"","code":"eval_pred_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine predictive parity of a model — eval_pred_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_pred_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine predictive parity of a model — eval_pred_parity","text":"list containing following elements: PPV_Group1: Positive Predictive Value first group PPV_Group2: Positive Predictive Value second group PPV_Diff: Difference Positive Predictive Value confidence intervals computed (confint = TRUE): PPV_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Predictive Value","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_stats_parity.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine statistical parity of a model — eval_stats_parity","title":"Examine statistical parity of a model — eval_stats_parity","text":"Examine statistical parity model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_stats_parity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine statistical parity of a model — eval_stats_parity","text":"","code":"eval_stats_parity(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_stats_parity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine statistical parity of a model — eval_stats_parity","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable, must binary group Name sensitive attribute probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_stats_parity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine statistical parity of a model — eval_stats_parity","text":"list containing following elements: PPR_Group1: Positive Prediction Rate first group PPR_Group2: Positive Prediction Rate second group PPR_Diff: Difference Positive Prediction Rate confidence intervals computed (confint = TRUE): PPR_Diff_CI: vector length 2 containing lower upper bounds 95% confidence interval difference Positive Prediction Rate","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_treatment_equality.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine treatment equality of a model — eval_treatment_equality","title":"Examine treatment equality of a model — eval_treatment_equality","text":"Examine treatment equality model","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_treatment_equality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine treatment equality of a model — eval_treatment_equality","text":"","code":"eval_treatment_equality(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2,   message = TRUE )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_treatment_equality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine treatment equality of a model — eval_treatment_equality","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome Name outcome variable group Name sensitive attribute probs Predicted probabilities cutoff Cutoff value predicted probabilities confint Logical indicating whether calculate confidence intervals bootstraps Number bootstraps use confidence intervals digits Number digits round results , default 2 message Whether print results, default TRUE","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_treatment_equality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine treatment equality of a model — eval_treatment_equality","text":"list containing following elements: False Negative / False Positive ratio Group 1 False Negative / False Positive ratio Group 2 Difference False Negative / False Positive ratio confidence intervals computed (confint = TRUE): vector length 2 containing lower upper bounds 95% confidence interval difference False Negative / False Positive ratio","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_variance.html","id":null,"dir":"Reference","previous_headings":"","what":"Examine variance of a model\n#' This function evaluates the variance in model\nperformance metrics across different groups. — eval_variance","title":"Examine variance of a model\n#' This function evaluates the variance in model\nperformance metrics across different groups. — eval_variance","text":"Examine variance model #' function evaluates variance model performance metrics across different groups.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_variance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Examine variance of a model\n#' This function evaluates the variance in model\nperformance metrics across different groups. — eval_variance","text":"","code":"eval_variance(   data,   outcome,   group,   probs,   cutoff = 0.5,   confint = TRUE,   bootstraps = 1000,   digits = 2 )"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_variance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Examine variance of a model\n#' This function evaluates the variance in model\nperformance metrics across different groups. — eval_variance","text":"data Data frame containing outcome, predicted outcome, group outcome Name outcome variable, must binary group Name group probs Name predicted outcome variable cutoff Threshold predicted outcome, default 0.5 confint Whether compute 95% confidence interval, default TRUE bootstraps Number bootstrap samples, default 1000 digits Number digits round results , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/eval_variance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Examine variance of a model\n#' This function evaluates the variance in model\nperformance metrics across different groups. — eval_variance","text":"data frame containing following elements: Metric: names metrics calculated. Values group. Variance: variance model confidence intervals computed (confint = TRUE): 95% CI: string providing lower upper bounds 95% confidence interval variance.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/get_all_metrics.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the all metrics at once — get_all_metrics","title":"Calculate the all metrics at once — get_all_metrics","text":"Calculate metrics ","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/get_all_metrics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the all metrics at once — get_all_metrics","text":"","code":"get_all_metrics(data, outcome, group, probs, cutoff = 0.5, digits = 2)"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/get_all_metrics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the all metrics at once — get_all_metrics","text":"data Data frame containing outcome, predicted outcome, sensitive attribute outcome name outcome variable, must binary group name sensitive attribute probs name predicted outcome variable cutoff threshold predicted outcome, default 0.5 digits number digits round result , default 2","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/get_all_metrics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the all metrics at once — get_all_metrics","text":"Data frame metrics","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/mimic.html","id":null,"dir":"Reference","previous_headings":"","what":"Clinical data from the MIMIC-II database for a case study on\nindwelling arterial catheters — mimic","title":"Clinical data from the MIMIC-II database for a case study on\nindwelling arterial catheters — mimic","text":"Indwelling Arterial Catheter Clinical dataset contains clinical data 1776 patients MIMIC-II clinical database. basis article: Hsu DJ, et al. association indwelling arterial catheters mortality hemodynamically stable patients respiratory failure: propensity score analysis. Chest, 148(6):1470–1476, Aug. 2015. dataset also used Raffa et al. Chapter 5 \"Data Analysis\" forthcoming book: Secondary Analysis Electronic Health Records, published Springer 2016.","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/mimic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clinical data from the MIMIC-II database for a case study on\nindwelling arterial catheters — mimic","text":"","code":"mimic"},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/mimic.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Clinical data from the MIMIC-II database for a case study on\nindwelling arterial catheters — mimic","text":"data frame 1776 rows 46 variables: aline_flg Integer, indicates IAC used (1 = yes, 0 = ) icu_los_day Double, length stay ICU (days) hospital_los_day Integer, length stay hospital (days) age Double, age baseline (years) gender_num Integer, patient gender (1 = male; 0 = female) weight_first Double, first weight (kg) bmi Double, patient BMI sapsi_first Integer, first SAPS score sofa_first Integer, first SOFA score service_unit Character, type service unit (FICU, MICU, SICU) service_num Integer, service numeric value (0 = MICU FICU, 1 = SICU) day_icu_intime Character, day week ICU admission day_icu_intime_num Integer, day week ICU admission (numeric) hour_icu_intime Integer, hour ICU admission (24hr clock) hosp_exp_flg Integer, death hospital (1 = yes, 0 = ) icu_exp_flg Integer, death ICU (1 = yes, 0 = ) day_28_flg Integer, death within 28 days (1 = yes, 0 = ) mort_day_censored Double, day post ICU admission censoring death (days) censor_flg Integer, censored death (0 = death, 1 = censored) sepsis_flg Integer, sepsis present (0 = , 1 = yes) chf_flg Integer, congestive heart failure (0 = , 1 = yes) afib_flg Integer, atrial fibrillation (0 = , 1 = yes) renal_flg Integer, chronic renal disease (0 = , 1 = yes) liver_flg Integer, liver disease (0 = , 1 = yes) copd_flg Integer, chronic obstructive pulmonary disease (0 = , 1 = yes) cad_flg Integer, coronary artery disease (0 = , 1 = yes) stroke_flg Integer, stroke (0 = , 1 = yes) mal_flg Integer, malignancy (0 = , 1 = yes) resp_flg Integer, respiratory disease (non-COPD) (0 = , 1 = yes) map_1st Double, mean arterial pressure (mmHg) hr_1st Integer, heart rate temp_1st Double, temperature (F) spo2_1st Integer, S_pO_2 (percent) abg_count Integer, arterial blood gas count (number tests) wbc_first Double, first white blood cell count (K/uL) hgb_first Double, first hemoglobin (g/dL) platelet_first Integer, first platelets (K/u) sodium_first Integer, first sodium (mEq/L) potassium_first Double, first potassium (mEq/L) tco2_first Double, first bicarbonate (mEq/L) chloride_first Integer, first chloride (mEq/L) bun_first Integer, first blood urea nitrogen (mg/dL) creatinine_first Double, first creatinine (mg/dL) po2_first Integer, first PaO_2 (mmHg) pco2_first Integer, first PaCO_2 (mmHg) iv_day_1 Double, input fluids IV day 1 (mL)","code":""},{"path":"https://jianhuig.github.io/FairnessTutorial/reference/mimic.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Clinical data from the MIMIC-II database for a case study on\nindwelling arterial catheters — mimic","text":"https://physionet.org/content/mimic2-iaccd/1.0/","code":""}]
