% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Fairness.R
\name{eval_cond_acc_equality}
\alias{eval_cond_acc_equality}
\title{Examine conditional use accuracy equality of a model}
\usage{
eval_cond_acc_equality(
  data,
  outcome,
  group,
  probs,
  cutoff = 0.5,
  confint = TRUE,
  bootstraps = 1000,
  digits = 2,
  message = TRUE
)
}
\arguments{
\item{data}{Data frame containing the outcome, predicted outcome, and
sensitive attribute}

\item{outcome}{Name of the outcome variable, it must be binary}

\item{group}{Name of the sensitive attribute}

\item{probs}{Name of the predicted outcome variable}

\item{cutoff}{Threshold for the predicted outcome, default is 0.5}

\item{confint}{Whether to compute 95\% confidence interval, default is TRUE}

\item{bootstraps}{Number of bootstrap samples, default is 1000}

\item{digits}{Number of digits to round the results to, default is 2}

\item{message}{Whether to print the results, default is TRUE}
}
\value{
A list containing the following elements:
\itemize{
\item PPV_Group1: Positive Predictive Value for the first group
\item PPV_Group2: Positive Predictive Value for the second group
\item PPV_Diff: Difference in Positive Predictive Value
\item npvGroup1: Negative Predictive Value for the first group
\item npvGroup2: Negative Predictive Value for the second group
\item npvDiff: Difference in Negative Predictive Value
If confidence intervals are computed (\code{confint = TRUE}):
\item PPV_Diff_CI: 95\% confidence interval for the difference in Positive
Predictive Value
\item npvDiff_CI: 95\% confidence interval for the difference in Negative
Predictive Value
}
}
\description{
Examine conditional use accuracy equality of a model
}
